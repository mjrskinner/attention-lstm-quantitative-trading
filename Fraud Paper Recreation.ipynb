{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4 selenium yfinance pandas webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = [row.find_all('td')[0].text.strip() for row in table.find_all('tr')[1:]]\n",
    "    return tickers\n",
    "\n",
    "def download_ticker_cik_mapping():\n",
    "    url = \"https://www.sec.gov/include/ticker.txt\"\n",
    "    response = requests.get(url)\n",
    "    ticker_cik_mapping = {line.split('\\t')[0]: line.split('\\t')[1] for line in response.text.splitlines()}\n",
    "    return ticker_cik_mapping\n",
    "\n",
    "def format_cik(cik):\n",
    "    return str(cik).zfill(10)  # Pad the CIK to ensure it is 10 digits long\n",
    "\n",
    "def map_sp500_tickers_to_cik():\n",
    "    sp500_tickers = fetch_sp500_tickers()\n",
    "    ticker_cik_mapping = download_ticker_cik_mapping()\n",
    "\n",
    "    sp500_ticker_cik_mapping = {}\n",
    "    for ticker in sp500_tickers:\n",
    "        cik = ticker_cik_mapping.get(ticker.lower())\n",
    "        if cik:\n",
    "            formatted_cik = format_cik(cik)\n",
    "            sp500_ticker_cik_mapping[ticker] = formatted_cik\n",
    "\n",
    "    return sp500_ticker_cik_mapping\n",
    "\"\"\"\n",
    "# Fetch and print the data\n",
    "sp500_ticker_cik_mapping = map_sp500_tickers_to_cik()\n",
    "print(sp500_ticker_cik_mapping)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tag: Liabilities\n",
      "Found tag: StockholdersEquity\n",
      "Found tag: NetIncomeLoss\n",
      "Found tag: RevenueFromContractWithCustomerExcludingAssessedTax\n",
      "Found tag: EarningsPerShareBasic\n",
      "Found tag: CommonStockSharesOutstanding\n",
      "Found tag: Assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe below code can be used to print the collected data, as needed.\\n\\nfor concept, data in financial_historicals.items():\\n    print(f\"\\n{concept}:\")\\n    for tag, unit, value, date in data:  # Corrected to handle four elements\\n        print(f\"Tag: {tag}, Unit: {unit}, Date: {date}, Value: {value}\")\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_company_xbrl_facts(cik):\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    headers = {'User-Agent': 'ADVANCE INDUSTRIES LLC: MR. MALCOLM JAMES RUTLEDGE SKINNER'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for CIK {cik}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_historical_data(data, concept_tags):\n",
    "    historical_values = {}\n",
    "    \n",
    "    for tag in concept_tags:\n",
    "        if tag in data['facts']['us-gaap']:\n",
    "            print(f\"Found tag: {tag}\")\n",
    "            tag_data = data['facts']['us-gaap'][tag]\n",
    "            for unit in tag_data['units']:\n",
    "                dates_seen = set()\n",
    "                try:\n",
    "                    # Reverse the list to start with the most recent entry\n",
    "                    for item in reversed(tag_data['units'][unit]):\n",
    "                        date = item.get('end', item.get('instant'))\n",
    "                        if date not in dates_seen:\n",
    "                            historical_values[(tag, unit, date)] = item['val']\n",
    "                            dates_seen.add(date)\n",
    "                except KeyError as e:\n",
    "                    print(f\"KeyError for tag {tag} under unit {unit}: {e}\")\n",
    "\n",
    "    # Convert the dictionary to a list of tuples for output\n",
    "    return [(tag, unit, date, value) for (tag, unit, date), value in historical_values.items()]\n",
    "\n",
    "def fetch_financial_historicals(cik):\n",
    "    xbrl_data = fetch_company_xbrl_facts(cik)\n",
    "    if xbrl_data:\n",
    "        concepts = {\n",
    "            'TotalLiabilities': [\n",
    "                'Liabilities'\n",
    "            ],\n",
    "            'ShareholdersEquity': [\n",
    "                'StockholdersEquity'\n",
    "            ],\n",
    "            'NetIncome': [\n",
    "                'NetIncomeLoss'\n",
    "            ],\n",
    "            'Revenue': [\n",
    "                'RevenueFromContractWithCustomerExcludingAssessedTax'\n",
    "            ],\n",
    "            'EarningsPerShareDiluted': [\n",
    "                'EarningsPerShareBasic'\n",
    "            ],\n",
    "            'WeightedAverageNumberOfDilutedSharesOutstanding': [\n",
    "                'CommonStockSharesOutstanding'\n",
    "            ],\n",
    "            'Assets':[\n",
    "                'Assets'\n",
    "            ]\n",
    "        }\n",
    "        historicals = {}\n",
    "        for concept_name, concept_tags in concepts.items():\n",
    "            historicals[concept_name] = get_historical_data(xbrl_data, concept_tags)\n",
    "        return historicals\n",
    "\"\"\"\n",
    "# Example usage for a specific company\n",
    "cik = '0000789019'  # Example: CIK for Microsoft\n",
    "financial_historicals = fetch_financial_historicals(cik)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "The below code can be used to print the collected data, as needed.\n",
    "\n",
    "for concept, data in financial_historicals.items():\n",
    "    print(f\"\\n{concept}:\")\n",
    "    for tag, unit, value, date in data:  # Corrected to handle four elements\n",
    "        print(f\"Tag: {tag}, Unit: {unit}, Date: {date}, Value: {value}\")\n",
    "\n",
    "\"\"\"\n",
    "# DISCLAIMER\n",
    "# This code assumes that within a series of duplicate dates,\n",
    "# the last entry (as ordered in the data source) is the most accurate.\n",
    "# This may not be appropriate for all data sets or analyses.\n",
    "# Users should verify the assumption against their data and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns all of the XBRL tags and the date ranges that are used.\n",
    "import requests\n",
    "\n",
    "def fetch_company_xbrl_tags(cik):\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    headers = {'User-Agent': 'Your User Agent Info'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for CIK {cik}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def extract_xbrl_tags_with_dates(xbrl_data):\n",
    "    tag_dates = {}\n",
    "    if xbrl_data and 'facts' in xbrl_data and 'us-gaap' in xbrl_data['facts']:\n",
    "        for tag, data in xbrl_data['facts']['us-gaap'].items():\n",
    "            dates = []\n",
    "            for unit in data['units'].values():\n",
    "                for item in unit:\n",
    "                    date = item.get('end', item.get('instant'))\n",
    "                    if date:\n",
    "                        dates.append(date)\n",
    "            if dates:\n",
    "                tag_dates[tag] = (min(dates), max(dates))\n",
    "    return tag_dates\n",
    "\"\"\"\n",
    "cik = '0000789019'  # Example: CIK for Microsoft\n",
    "xbrl_data = fetch_company_xbrl_tags(cik)\n",
    "\n",
    "if xbrl_data:\n",
    "    xbrl_tags_dates = extract_xbrl_tags_with_dates(xbrl_data)\n",
    "    for tag, dates in sorted(xbrl_tags_dates.items()):\n",
    "        print(f\"Tag: {tag}, Date Range: {dates[0]} to {dates[1]}\")\n",
    "else:\n",
    "    print(\"No XBRL data found.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': 'Aug. 31, 2020', 'ratio': '4:1'}\n",
      "{'date': 'Jun. 09, 2014', 'ratio': '7:1'}\n",
      "{'date': 'Feb. 28, 2005', 'ratio': '2:1'}\n",
      "{'date': 'Jun. 21, 2000', 'ratio': '2:1'}\n",
      "{'date': 'Jun. 16, 1987', 'ratio': '2:1'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_splits_with_selenium(ticker):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    url = f'https://seekingalpha.com/symbol/{ticker}/splits'\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(10)\n",
    "\n",
    "    # Scroll the page to ensure all elements are loaded\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    splits_table = soup.find('tbody', {'data-test-id': 'table-body'})\n",
    "    splits = []\n",
    "    \n",
    "    if splits_table:\n",
    "        for row in splits_table.find_all('tr'):\n",
    "            date = row.find('th').get_text(strip=True)\n",
    "            ratio = row.find('td').get_text(strip=True)\n",
    "            splits.append({'date': date, 'ratio': ratio})\n",
    "\n",
    "    browser.quit()\n",
    "    return splits\n",
    "\n",
    "# Example usage with ticker \"AAPL\"\n",
    "\"\"\"\n",
    "splits_data = get_splits_with_selenium(\"AAPL\")\n",
    "for split in splits_data:\n",
    "    print(split)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for MSFT saved to msft_stock_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-02</th>\n",
       "      <td>17.203354</td>\n",
       "      <td>44487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-05</th>\n",
       "      <td>17.635780</td>\n",
       "      <td>67333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-06</th>\n",
       "      <td>17.698463</td>\n",
       "      <td>46950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-07</th>\n",
       "      <td>17.679655</td>\n",
       "      <td>54298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-08</th>\n",
       "      <td>17.648319</td>\n",
       "      <td>58810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-06</th>\n",
       "      <td>356.529999</td>\n",
       "      <td>23828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-07</th>\n",
       "      <td>360.529999</td>\n",
       "      <td>25833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-08</th>\n",
       "      <td>363.200012</td>\n",
       "      <td>26767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-09</th>\n",
       "      <td>360.690002</td>\n",
       "      <td>24847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10</th>\n",
       "      <td>369.670013</td>\n",
       "      <td>28042100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close    Volume\n",
       "Date                            \n",
       "2004-01-02   17.203354  44487700\n",
       "2004-01-05   17.635780  67333700\n",
       "2004-01-06   17.698463  46950800\n",
       "2004-01-07   17.679655  54298200\n",
       "2004-01-08   17.648319  58810800\n",
       "...                ...       ...\n",
       "2023-11-06  356.529999  23828300\n",
       "2023-11-07  360.529999  25833900\n",
       "2023-11-08  363.200012  26767800\n",
       "2023-11-09  360.690002  24847300\n",
       "2023-11-10  369.670013  28042100\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def download_market_data(ticker, start_date, end_date, file_name):\n",
    "    \"\"\"\n",
    "    Downloads market data for a given ticker and saves it to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    ticker (str): The ticker symbol of the stock or index.\n",
    "    start_date (str): The start date for the data in YYYY-MM-DD format.\n",
    "    end_date (str): The end date for the data in YYYY-MM-DD format.\n",
    "    file_name (str): The name of the file to save the data to.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The data for the specified ticker and date range.\n",
    "    \"\"\"\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    data = data[['Adj Close', 'Volume']]  # Select adjusted close and volume\n",
    "    data.to_csv(file_name)  # Save to CSV\n",
    "    print(f\"Data for {ticker} saved to {file_name}\")\n",
    "    return data\n",
    "\"\"\"\n",
    "# Example usage\n",
    "download_market_data('MSFT', '2004-01-01', '2023-11-11', 'msft_stock_data.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def adjust_eps_for_splits(eps_df, splits_data):\n",
    "    \"\"\"\n",
    "    Adjusts the EPS for a stock based on its split history.\n",
    "\n",
    "    Parameters:\n",
    "    eps_df (DataFrame): A DataFrame with columns 'Date' and 'EPS' where 'Date' is the reporting date.\n",
    "    splits_data (list of dicts): A list where each dict contains 'date' and 'ratio' keys for each stock split.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The adjusted EPS DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert split ratios from strings to numerical values (e.g., '2:1' becomes 2.0)\n",
    "    for split in splits_data:\n",
    "        split['date'] = pd.to_datetime(split['date'])\n",
    "        split_ratio = split['ratio'].split(':')\n",
    "        split['ratio'] = float(split_ratio[0]) / float(split_ratio[1])\n",
    "    \n",
    "    # Sort the splits by date\n",
    "    splits_data.sort(key=lambda x: x['date'])\n",
    "    \n",
    "    # Adjust the EPS\n",
    "    for index, row in eps_df.iterrows():\n",
    "        # Find splits that occurred after the EPS report date\n",
    "        for split in splits_data:\n",
    "            if split['date'] > row['Date']:\n",
    "                row['EPS'] /= split['ratio']\n",
    "    \n",
    "    return eps_df\n",
    "\n",
    "# Example usage:\n",
    "# Assume eps_df is your dataframe with EPS data and splits_data is your list of split information.\n",
    "# adjusted_eps_df = adjust_eps_for_splits(eps_df, splits_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manufacture financial ratios as needed\n",
    "# function should also get the dates for which the financial ratio is what it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stock ticker\n",
    "# get it's cik\n",
    "# get fundamental data, store in pandas dataframe\n",
    "# get split dates for stock\n",
    "# adjust P/E based on split\n",
    "# get the rest of the financial ratios together\n",
    "# merge and fill fundamental data with adjusted close and volume"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
